---
layout: page
title: Samin Yeasar Arnob
subtitle: 
---

<img src="https://imgur.com/IRBaiqh.png" width="120" height="50"> <img src="https://imgur.com/EQKabmk.png" width="120" height="65"> <img src="https://imgur.com/kNigIqj.png" width="140" height="140"> <img src="https://imgur.com/eWTBidl.png" width="60" height="55"> 

### Latest News
* **September 2025** I'm joining **Cohere** at the RL team, will be working on next-gen agentic system.
* **July 2025.** Extented version of the paper "**Sparse Adapters**" got accepted in **COLM 2025**! ðŸŽ‰
   * [Paper](https://arxiv.org/abs/2507.07140),[Code](https://github.com/SaminYeasar/sparse_adapter/tree/main/projects/sparse_finetuning)  
* **March 2025.** "**Exploring Sparse Adapters for Scalable Merging of Parameter Efficient Experts**" got accepted in **ICLR 2025, Workshop on MCDC**! ðŸŽ‰
   * [Paper](https://openreview.net/forum?id=8wt2eKkVe6), [Poster](https://drive.google.com/file/d/16TZNQRxT-C9_4TPIQ0M9Kn-tocV8XMHP/view?usp=sharing), [Code](https://github.com/SaminYeasar/sparse_adapter/tree/main/projects/sparse_finetuning)
   * **Keywords**: Sparse adapter, Parameter-efficient finetuning, Model merging, LLM
   * **TL;DR**: We explore sparse adapters as a simpler and more effective building block for modular, parameter-efficient architectures, demonstrating superior model merging performance at scale.
* **March 2025.** "**Sparse-Reg: Improving Sample Complexity of Offline Reinforcement Learning using Sparse Regularization.**" got accepted in **RLDM 2025**! ðŸŽ‰
   * [Paper](https://drive.google.com/file/d/1zIc-OOdd2R9a5LlMjF3KyiICZ84urLdS/view?usp=sharing), [Code](https://github.com/SaminYeasar/sparse_reg)
   * **Keywords**: Offline Reinforcement Learning, Sparsity, Regularization, Sample Complexity, Continuous Control.
   * **TL;DR**: We introduce "Sparse-Reg," a regularization technique that mitigates overfitting in offline reinforcement learning with small datasets, improving performance in continuous control tasks.
* **September 2024.** "**Efficient Reinforcement Learning by Discovering Neural Pathways**" got accepted in **NeurIPS 2024**! ðŸŽ‰
   * [Project Page](https://neural-pathways.github.io), [Paper](https://proceedings.neurips.cc/paper_files/paper/2024/hash/216f4cd12cfd69d46770bb2b491ae24b-Abstract-Conference.html), [Code](https://github.com/SaminYeasar/DAPD)
   * **Keywords**: Energy Efficient AI, Parameter Efficient, Neural Pathways, Continuous Control, Online Reinforcement Learning, Offline Reinforcement Learning, Multitask Reinforcement Learning.
  * **TL;DR**: To improve energy efficiency and reduce the carbon footprint, we propose Neural Pathway to efficiently use the network parameter space for reinforcement learning.


---

I am a visiting student-researcher at **Microsoft Research, Montreal** and doing a Ph.D. in Computer Science at **McGill University** and [Mila Quebec AI Institute][1] working with [Dr. Doina Precup][3].

My research focus is on "**Improving the Learning Capacity and Parameter Efficient training for RL and LLMs**". I am working on efficiently using neural networks where we take inspiration from the human brain, using multiple specialized pathways through a single network, with each pathway focusing on a single task. This is an alternate way to "routing" and a mixture of expert structures that can be added to LLM. 

I'm interested in the ***Mixture of experts*** (MoE), ***Parameter-efficient finetuning*** (peft) in LLM, ***Preference fine-tuning*** using Reinforcement Learning (RL), LLM ***alignment***, ***Improving mergability of a mixture of experts***.

---
Prior I did applied research internships at **Microsoft Research, New York** (summer 2023, host: John Lanford, Alex Lamb), **Ubisoft La Forge, Montreal** (2021-2022, host: Joshua Romoff), **Mila Quebec AI Institute** (2019, host: Doina Precup)

I completed Master's in Electrical and Computer Engineering at McGill University.  My master's research was on "Adversarial Inverse Reinforcement Learning" under the supervision of [Dr. Aditya Mahajan][4] at [Centre for Intelligent Machine (CIM)][5].


[1]:https://mila.quebec/
[2]:http://rl.cs.mcgill.ca/
[3]:https://www.linkedin.com/in/doina-precup-1ba61314/
[4]:http://www.ece.mcgill.ca/~amahaj1/
[5]:https://www.mcgill.ca/cim/


 
### News
* **March 2024.** I'm joined **Microsoft Research**, Montreal as part-time research intern. I will be working on sparse-adapters for efficient fine-tuning and model merging at scale for LLMs.
* **May, 2023- Aug, 2023.** I worked at **Microsoft Research**, New York as **Research Intern**, on an Appiled RL project with John Langford and Alex Lamb.
* **October, 2021.** Two papers got accepted in **Offline Reinforcement Learning Workshop, NeurIPS 2021**
  - "Importance of Empirical Sample Complexity Analysis for Offline Reinforcement Learning" - [Paper](https://offline-rl-neurips.github.io/2021/pdf/38.pdf)
  - "Single-Shot Pruning for Offline Reinforcement Learning" - [Paper](https://offline-rl-neurips.github.io/2021/pdf/27.pdf)
* **Sep, 2021- Aug 2022.** I have worked at **Ubisoft**, Montreal with Joshua Romoff as **Research Intern**.
* **June, 2020.** "Off-Policy Adversarial Inverse Reinforcement Learning" got accepted in **Lifelong Learning workshop, ICML 2020**.
    [Paper](https://openreview.net/forum?id=9mp5d073IhX), [Code](https://github.com/SaminYeasar/Off_Policy_Adversarial_Inverse_Reinforcement_Learning),[Talk](https://www.youtube.com/watch?v=PK3byu61JKI&ab_channel=SaminYeasarArnob)
* **January, 2020.** I have started my **Ph.D.** at **McGill University**.
* **June, 2019.** I joined **Mila** as **Research Intern**.
* **June, 2019.** "Doubly Robust Estimators in Off-Policy Actor-Critic Algorithms" got accepted for **spotlight** presentation at **RLDM 2019**
* **January, 2018.** I started my Master's at McGill University.


### Research Interest

* Reinforcement Learning: Improving LLM, Imitation Learning, Offline  Reinforcement Learning, Multitask Learning, Representation learning
* Large Language Model: Mixture of experts (MoE),  Improving mergability of a mixture of experts, Parameter-efficient finetuning (peft), Preference fine-tuning, LLM alignment
